{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HyperTone - Fit.ipynb",
      "provenance": [],
      "mount_file_id": "1RrnkabZVmWduNMQ_myL12ZCz2YJMc-Qi",
      "authorship_tag": "ABX9TyPjww6LKsSAqwhu8pWGllSl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stoianmihail/HyperTone/blob/main/HyperTone_Fit-100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTVG1bmuFKiE"
      },
      "source": [
        "# lstm model\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import BatchNormalization as BatchNorm\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOOwoBDuGDDg"
      },
      "source": [
        "hypertone_dir = '/content/drive/MyDrive/hypertone/'\n",
        "preprocessing_dir = hypertone_dir + 'preprocessing/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXHHME_wGQyy"
      },
      "source": [
        "import os\n",
        "def get_files(path):\n",
        "  ret = []\n",
        "  for root, _, files in os.walk(path, topdown=False):\n",
        "    for file in files:\n",
        "      ret.append({'path': os.path.join(root,file), 'data' : None})\n",
        "  return ret"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZl4DC3qFfn-",
        "outputId": "b126281c-33f3-41a6-97b1-6a0deeba8621"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "def compress(a):\n",
        "  return a[np.logical_or(np.insert(np.diff(a).astype(bool), 0, True), np.asarray(a, dtype=bool))]\n",
        "\n",
        "def parse_songs():\n",
        "  songs = {}\n",
        "  for file in get_files(preprocessing_dir):\n",
        "    if '.pkl' not in file['path']:\n",
        "      continue \n",
        "    song_name = os.path.basename(file['path']).replace('.pkl', '')\n",
        "    x = pd.read_pickle(file['path'])\n",
        "    compressed = compress(np.asarray(x[1]))\n",
        "    compressed = compressed[compressed != 0]\n",
        "\n",
        "    # TODO: this is only for these recordings\n",
        "    # TODO: Next time, also save the label!\n",
        "    tone = int(x[2])\n",
        "    if tone == 1 or tone == 8:# or tone == 5 or tone == 3:\n",
        "      songs[song_name] = {'x': compressed, 'y': tone, 'raw' : x[0]}\n",
        "  return songs\n",
        "\n",
        "songs = parse_songs()\n",
        "print(songs['1. 9 Ceea ce eşti mai cinstită']['x'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1 -1  1 -1  1 -1  1  1  1  1 -1  1  5  1  1  1  1  1 -1 -1 -1 -1 -1  8\n",
            "  1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1 -1 -1 -2 -1  1  1  1  1  1  1\n",
            " -1  1 -1  1 -7  1  1  4  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -3\n",
            "  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1 -1 -1 -3 -1 -1\n",
            " -1 -1  4  1  1 -1 -1 -1 -1 -1 -2  1  4 -1 -1  7  1  1  1 -1 -1 -1  1 -1\n",
            " -1 -1 -1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -4  1  1  1  1\n",
            "  1  1  4  1  1  1  1 -1 -1 -1  3  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1  1  1  1  1 -5  1  1  1  1  1  1  1  1  1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1 -1 -1 -3 -1 -1 -1 -1  1  1  1\n",
            "  1  1  1 -1 -1 -1 -1 -1 -1  1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ATMdJH8GlQ-"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "kSequenceLength = 128\n",
        "\n",
        "def update(y):\n",
        "  if y == 1:\n",
        "    return 0\n",
        "  return 1\n",
        "\n",
        "def padarray(A, size):\n",
        "    t = size - len(A)\n",
        "    return np.pad(A, pad_width=(0, t), mode='constant')\n",
        "\n",
        "def generate_training_sequences(seq_len=kSequenceLength):\n",
        "  # load songs and map them to int\n",
        "  songs_ = parse_songs()\n",
        "\n",
        "  inputs, targets = [], []\n",
        "  for song in songs_:\n",
        "    x = songs_[song]['x']\n",
        "    y = songs_[song]['y']\n",
        "    y = update(y)\n",
        "    #x[np.insert(np.diff(x).astype(np.bool), 0, True)]\n",
        "    if len(x) < seq_len:\n",
        "      x = padarray(x, seq_len)\n",
        "    num_sequences = len(x) - seq_len + 1\n",
        "    for index in range(num_sequences):\n",
        "      inputs.append(np.asarray(x[index : index + seq_len]).reshape((seq_len, 1)))\n",
        "      targets.append(y)\n",
        "\n",
        "  # one-hot encode the sequences  \n",
        "  # inputs size: (# of sequences, sequence length, max(diff))\n",
        "  #inputs = keras.utils.to_categorical(inputs, num_classes=kOutputUnits)\n",
        "  inputs = np.asarray(inputs)\n",
        "  print(inputs.shape)\n",
        "  #n = len(inputs)\n",
        "  #inputs = np.reshape(inputs, (n, seq_len, 1))\n",
        "  # normalize input\n",
        "  #inputs = inputs / float(kOutputUnits)\n",
        "  \n",
        "  targets = keras.utils.to_categorical(targets, num_classes=2)\n",
        "  print(targets.shape)\n",
        "  print(f\"There are {len(inputs)} sequences.\")\n",
        "  return inputs, targets"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhu01Zj2HNKP",
        "outputId": "1a8d6569-02d8-44a9-ae24-6cd7ef553385"
      },
      "source": [
        "generate_training_sequences()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11448, 128, 1)\n",
            "(11448, 2)\n",
            "There are 11448 sequences.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[ 1],\n",
              "         [-1],\n",
              "         [ 1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [ 1],\n",
              "         [ 1]],\n",
              " \n",
              "        [[-1],\n",
              "         [ 1],\n",
              "         [-1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [ 1],\n",
              "         [ 1]],\n",
              " \n",
              "        [[ 1],\n",
              "         [-1],\n",
              "         [ 1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [ 1],\n",
              "         [ 1]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 1],\n",
              "         [ 1],\n",
              "         [-1],\n",
              "         ...,\n",
              "         [-1],\n",
              "         [ 1],\n",
              "         [ 1]],\n",
              " \n",
              "        [[ 1],\n",
              "         [-1],\n",
              "         [-1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [ 1],\n",
              "         [-1]],\n",
              " \n",
              "        [[-1],\n",
              "         [-1],\n",
              "         [-1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [-1],\n",
              "         [ 1]]]), array([[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        ...,\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vixU6TGFKWo",
        "outputId": "0bafb847-2413-446c-b209-4e8221d7d5de"
      },
      "source": [
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset():\n",
        "  # load all train\n",
        "  trainX, trainy = generate_training_sequences()\n",
        "  print(trainX.shape, trainy.shape)\n",
        "\n",
        "  # load all test\n",
        "  testX, testy = generate_training_sequences()\n",
        "  \n",
        "  # zero-offset class values\n",
        "  #trainy = trainy\n",
        "  #testy = testy\n",
        "  \n",
        "  # one hot encode y\n",
        "  print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "  return trainX, trainy, trainX, trainy\n",
        "\n",
        "load_dataset()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11448, 128, 1)\n",
            "(11448, 2)\n",
            "There are 11448 sequences.\n",
            "(11448, 128, 1) (11448, 2)\n",
            "(11448, 128, 1)\n",
            "(11448, 2)\n",
            "There are 11448 sequences.\n",
            "(11448, 128, 1) (11448, 2) (11448, 128, 1) (11448, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[ 1],\n",
              "         [-1],\n",
              "         [ 1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [ 1],\n",
              "         [ 1]],\n",
              " \n",
              "        [[-1],\n",
              "         [ 1],\n",
              "         [-1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [ 1],\n",
              "         [ 1]],\n",
              " \n",
              "        [[ 1],\n",
              "         [-1],\n",
              "         [ 1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [ 1],\n",
              "         [ 1]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 1],\n",
              "         [ 1],\n",
              "         [-1],\n",
              "         ...,\n",
              "         [-1],\n",
              "         [ 1],\n",
              "         [ 1]],\n",
              " \n",
              "        [[ 1],\n",
              "         [-1],\n",
              "         [-1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [ 1],\n",
              "         [-1]],\n",
              " \n",
              "        [[-1],\n",
              "         [-1],\n",
              "         [-1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [-1],\n",
              "         [ 1]]]), array([[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        ...,\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.]], dtype=float32), array([[[ 1],\n",
              "         [-1],\n",
              "         [ 1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [ 1],\n",
              "         [ 1]],\n",
              " \n",
              "        [[-1],\n",
              "         [ 1],\n",
              "         [-1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [ 1],\n",
              "         [ 1]],\n",
              " \n",
              "        [[ 1],\n",
              "         [-1],\n",
              "         [ 1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [ 1],\n",
              "         [ 1]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 1],\n",
              "         [ 1],\n",
              "         [-1],\n",
              "         ...,\n",
              "         [-1],\n",
              "         [ 1],\n",
              "         [ 1]],\n",
              " \n",
              "        [[ 1],\n",
              "         [-1],\n",
              "         [-1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [ 1],\n",
              "         [-1]],\n",
              " \n",
              "        [[-1],\n",
              "         [-1],\n",
              "         [-1],\n",
              "         ...,\n",
              "         [ 1],\n",
              "         [-1],\n",
              "         [ 1]]]), array([[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        ...,\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr4RxxQ_FEwy",
        "outputId": "c1cf482f-a145-43df-d96a-2bf56d6b3511"
      },
      "source": [
        "kNumEpochs = 15\n",
        "kBatchSize = 64\n",
        "\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy, verbose=True):\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  print(n_timesteps, n_features, n_outputs)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(100, input_shape=(n_timesteps, n_features)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(BatchNorm())\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "\n",
        "  # model.add(LSTM(\n",
        "  #   128,\n",
        "  #   input_shape=(n_timesteps, n_features),\n",
        "  #   recurrent_dropout=0.3,\n",
        "  #   return_sequences=True\n",
        "  # ))\n",
        "  # model.add(LSTM(64))\n",
        "  # model.add(BatchNorm())\n",
        "  # model.add(Dropout(0.3))\n",
        "  # model.add(Dense(n_outputs, activation='softmax'))\n",
        "  # model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "  # fit network\n",
        "  model.fit(trainX, trainy, epochs=kNumEpochs, batch_size=kBatchSize, verbose=verbose)\n",
        "  # evaluate model\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=kBatchSize, verbose=False)\n",
        "  return accuracy\n",
        "\n",
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "  print(scores)\n",
        "  m, s = mean(scores), std(scores)\n",
        "  print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        "\n",
        "# run an experiment\n",
        "def run_experiment(repeats=1):\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()#'/content/drive/MyDrive/hypertone/preprocessing')\n",
        "  print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "  # repeat experiment\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  # summarize results\n",
        "  summarize_results(scores)\n",
        "\n",
        "# run the experiment\n",
        "run_experiment()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11448, 128, 1)\n",
            "(11448, 2)\n",
            "There are 11448 sequences.\n",
            "(11448, 128, 1) (11448, 2)\n",
            "(11448, 128, 1)\n",
            "(11448, 2)\n",
            "There are 11448 sequences.\n",
            "(11448, 128, 1) (11448, 2) (11448, 128, 1) (11448, 2)\n",
            "(11448, 128, 1) (11448, 2) (11448, 128, 1) (11448, 2)\n",
            "128 1 2\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_16 (LSTM)               (None, 100)               40800     \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 64)                6464      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 47,794\n",
            "Trainable params: 47,594\n",
            "Non-trainable params: 200\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "179/179 [==============================] - 8s 33ms/step - loss: 0.7008 - accuracy: 0.5522\n",
            "Epoch 2/15\n",
            "179/179 [==============================] - 6s 33ms/step - loss: 0.6380 - accuracy: 0.6267\n",
            "Epoch 3/15\n",
            "179/179 [==============================] - 6s 33ms/step - loss: 0.5651 - accuracy: 0.7033\n",
            "Epoch 4/15\n",
            "179/179 [==============================] - 6s 33ms/step - loss: 0.4653 - accuracy: 0.7765\n",
            "Epoch 5/15\n",
            "179/179 [==============================] - 6s 33ms/step - loss: 0.3207 - accuracy: 0.8635\n",
            "Epoch 6/15\n",
            "179/179 [==============================] - 6s 33ms/step - loss: 0.1817 - accuracy: 0.9285\n",
            "Epoch 7/15\n",
            "179/179 [==============================] - 6s 33ms/step - loss: 0.0844 - accuracy: 0.9712\n",
            "Epoch 8/15\n",
            "179/179 [==============================] - 6s 33ms/step - loss: 0.0444 - accuracy: 0.9857\n",
            "Epoch 9/15\n",
            "179/179 [==============================] - 6s 33ms/step - loss: 0.0393 - accuracy: 0.9876\n",
            "Epoch 10/15\n",
            "179/179 [==============================] - 6s 33ms/step - loss: 0.0203 - accuracy: 0.9934\n",
            "Epoch 11/15\n",
            "179/179 [==============================] - 6s 33ms/step - loss: 0.0509 - accuracy: 0.9835\n",
            "Epoch 12/15\n",
            "179/179 [==============================] - 6s 33ms/step - loss: 0.0329 - accuracy: 0.9899\n",
            "Epoch 13/15\n",
            "179/179 [==============================] - 6s 33ms/step - loss: 0.0061 - accuracy: 0.9984\n",
            "Epoch 14/15\n",
            "179/179 [==============================] - 6s 32ms/step - loss: 0.0085 - accuracy: 0.9972\n",
            "Epoch 15/15\n",
            "179/179 [==============================] - 6s 32ms/step - loss: 0.0012 - accuracy: 0.9998\n",
            ">#1: 100.000\n",
            "[100.0]\n",
            "Accuracy: 100.000% (+/-0.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL3Ku4a8IwBv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}